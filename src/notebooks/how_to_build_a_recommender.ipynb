{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build a Recommender in Python\n",
    "\n",
    "\n",
    "### by Maria Dominguez (aka Chi) & João Ascensão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "#### Leveraging Python's data stack to build a music recommender, not unlike Spotify's Discover Weekly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender Systems:\n",
    "\n",
    "Recommender systems present *items* that are likely to interest the *user*, by comparing the user's profile (inferred from *data*) to reference characteristics.\n",
    "\n",
    "Thus, our general framework contains three main components:\n",
    "\n",
    "* **Users**: the people in our system, that generate data and will receive item recommendations\n",
    "* **Items**: the things in our system, with which the users interact and that we want to recommend to them\n",
    "* **Data**: the different ways an user can express opinions about our items, i.e. where users and items meet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Recommendations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word out there](https://hackernoon.com/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe), is that Spotify's Discover Weekly mixes together two well-known types of recommenders:\n",
    "\n",
    "* **Content-based filtering**: combines data (e.g. clicks, playcounts, ) and item attributes (e.g. content, text, tags, metadata) to create *user profiles*\n",
    "\n",
    "    * **Natural language processing (NLP)**: representing artists and tracks with features extracted from text (e.g. description, comments, tags)\n",
    "    * **Audio models**: extracting features by analyzing the raw audio tracks.\n",
    "    \n",
    "    \n",
    "* **Collaborative filtering**: analyzes user historical behaviour to find similarities across pairs of users and/or items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Recommender:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from the lastfm dataset, our prototype will include:\n",
    "\n",
    "* A content-based filtering branch, based on processing the tags assigned by the users to each artist (no raw audio analysis today, bummer)\n",
    "* A collaborative-filtering approach, using user/artist playcounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based filtering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a file we prepared containing a bunch of tags relative to the artists in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_cols = ['user_id', 'artist_id', 'artist_name', 'play_counts']\n",
    "train = pd.read_csv(\"../data/lastfm/train.csv\", header=None, names=train_cols)\n",
    "# We checked, there are no missing values\n",
    "train.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_cols = ['artist_id', 'tag']\n",
    "tags = pd.read_csv(\"../data/lastfm/tags.csv\", header=None, names=tags_cols)\n",
    "print(\"There are {} tags missing. We must drop them.\".format(tags['tag'].isnull().sum()))\n",
    "tags = tags.dropna()\n",
    "tags.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find out which artists appear in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_intersect = np.intersect1d(tags['artist_id'].values, train['artist_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "\n",
    "tags = tags.set_index('artist_id').loc[artists_intersect].reset_index()\n",
    "tags['artist_id'] = le1.fit_transform(tags['artist_id'])\n",
    "tags = tags.set_index('artist_id').sort_index().reset_index()\n",
    "tags.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index('artist_id').loc[artists_intersect].reset_index()\n",
    "train['artist_id'] = le1.fit_transform(train['artist_id'])\n",
    "train['user_id'] = le2.fit_transform(train['user_id'])\n",
    "train = train.set_index('artist_id').sort_index().reset_index()\n",
    "train.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building item profiles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check if we have the same number of artists in both dataframes and how many unique tags we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_artists_with_tags = len(tags['artist_id'].unique())\n",
    "total_artists_with_playcounts = len(train['artist_id'].unique())\n",
    "if total_artists_with_tags == total_artists_with_playcounts:\n",
    "    print(\"Both dataframes contain the same number of unique artists, a job well done!\\n\")\n",
    "\n",
    "total_tags = len(tags['tag'].unique())\n",
    "print(\"There are {} unique tags for {} unique artists.\".format(total_tags, \n",
    "                                                               total_artists_with_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the most common tags, let's check the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_count = tags['tag'].value_counts()\n",
    "print(tags_count[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a random sample of 5 of the most obscure tags, using a random permutation over tags with one appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_count_one = tags_count[tags_count == 1]\n",
    "tags_count_one[np.random.permutation(len(tags_count_one))][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is, instead of a single tag per row, we need to have *collection* of tags per artist. We can accomplish this by concatenating all tags per artist.\n",
    "\n",
    "The result resembles a document per artist, in which some words (e.g. `alternative`), previously associated with particular tags, appear now more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = tags.groupby('artist_id')['tag'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to split strings with \"-\" into multiple words (like `alternative-rock` to `alternative rock`). \n",
    "\n",
    "**At this point, do things that don't scale, like manually going through a good sample of documents to spot possible strategies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags['tag'] = tags['tag'].str.replace(\"-\", \" \")\n",
    "tags.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply something like a bag of words strategy with these documents, transforming them into *keyword vectors*.\n",
    "\n",
    "A reasonable strategy might be splitting strings into separe words, as there are many variations for each gender (e.g. rock and alternative rock).\n",
    "\n",
    "Stemming, on the other hand, reduces words to their most basic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "# Add language stemmer to TfidfVectorizer by overriding build_analyzer()\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        stemmer = EnglishStemmer()\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=3, analyzer='word', \n",
    "                                    stop_words='english', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(tags['tag'])\n",
    "\n",
    "vocabulary = list(vectorizer.vocabulary_)\n",
    "print(\"Sample of the vocabulary {}.\".format(vocabulary[:9]))\n",
    "print(\"There are {} unique tags.\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tag_matrix = vectorizer.transform(tags['tag'])\n",
    "print(item_tag_matrix.shape, type(item_tag_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "\n",
    "rows = le1.fit_transform(train['user_id'])\n",
    "cols = le2.fit_transform(train['artist_id'])\n",
    "data = train['play_counts'].values\n",
    "\n",
    "# [row_ind[k], col_ind[k]] = data[k]\n",
    "m = csr_matrix((data, (rows, cols)), shape=(rows.max()+1, cols.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = m.dot(item_tag_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colaborative-filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
